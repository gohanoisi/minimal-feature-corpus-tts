{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleTTS2 評価音声生成\n",
    "\n",
    "学習済みモデルを使って、共通評価文で音声を生成し、音声品質を比較します。\n",
    "\n",
    "## 評価文\n",
    "\n",
    "以下の3つの評価文を使用:\n",
    "1. \"こんにちは、今日は良い天気ですね。\"\n",
    "2. \"人工知能の発展により、音声合成技術も進化しています。\"\n",
    "3. \"この文章を自然な音声で読み上げます。\"\n",
    "\n",
    "## 出力\n",
    "\n",
    "各データセットごとに以下を生成:\n",
    "- `outputs/eval_audio/jvs002_{dataset_name}/eval_001.wav`\n",
    "- `outputs/eval_audio/jvs002_{dataset_name}/eval_002.wav`\n",
    "- `outputs/eval_audio/jvs002_{dataset_name}/eval_003.wav`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "from munch import Munch\n",
    "import soundfile as sf\n",
    "\n",
    "# プロジェクトルートを取得\n",
    "# プロジェクトルートを取得\n",
    "# notebooks/styletts2/ から実行される場合: 2階層上がる\n",
    "# notebooks/ から実行される場合: 1階層上がる\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"styletts2\":\n",
    "    PROJECT_ROOT = current_dir.parent.parent  # notebooks/styletts2 -> notebooks -> project_root\n",
    "elif current_dir.name == \"notebooks\":\n",
    "    PROJECT_ROOT = current_dir.parent  # notebooks -> project_root\n",
    "else:\n",
    "    PROJECT_ROOT = current_dir  # プロジェクトルートから直接実行\n",
    "STYLETTS2_DIR = PROJECT_ROOT / \"StyleTTS2\"\n",
    "OUTPUT_ROOT = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "# StyleTTS2をパスに追加\n",
    "if STYLETTS2_DIR.exists() and str(STYLETTS2_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(STYLETTS2_DIR))\n",
    "\n",
    "# データセット名のリスト\n",
    "DATASETS = [\"phone_min4\", \"feat_top10\", \"full100\"]\n",
    "\n",
    "# 評価文\n",
    "EVAL_TEXTS = [\n",
    "    \"こんにちは、今日は良い天気ですね。\",\n",
    "    \"人工知能の発展により、音声合成技術も進化しています。\",\n",
    "    \"この文章を自然な音声で読み上げます。\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"評価音声生成\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nプロジェクトルート: {PROJECT_ROOT}\")\n",
    "print(f\"StyleTTS2ディレクトリ: {STYLETTS2_DIR}\")\n",
    "print(f\"データセット: {DATASETS}\")\n",
    "print(f\"\\n評価文数: {len(EVAL_TEXTS)}\")\n",
    "for i, text in enumerate(EVAL_TEXTS, 1):\n",
    "    print(f\"  {i}. {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバイス設定\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nデバイス: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StyleTTS2モジュールのインポート\n",
    "from models import *\n",
    "from utils import *\n",
    "from text_utils import TextCleaner\n",
    "\n",
    "textcleaner = TextCleaner()\n",
    "\n",
    "print(\"✓ StyleTTS2モジュールをインポートしました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音素化関数\n",
    "from phonemizer import phonemize\n",
    "\n",
    "def phonemize_japanese(text: str) -> str:\n",
    "    \"\"\"日本語テキストをeSpeak-NGで音素化\"\"\"\n",
    "    try:\n",
    "        phonemized = phonemize(\n",
    "            text,\n",
    "            backend='espeak',\n",
    "            language='ja',\n",
    "            strip=True,\n",
    "            preserve_punctuation=True,\n",
    "            with_stress=False,\n",
    "            njobs=1\n",
    "        )\n",
    "        return phonemized.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"音素化エラー: {e}\")\n",
    "        return text\n",
    "\n",
    "print(\"✓ 音素化関数を定義しました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル読み込み関数\n",
    "def load_model(dataset_name: str):\n",
    "    \"\"\"\n",
    "    学習済みモデルを読み込む\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: データセット名\n",
    "        \n",
    "    Returns:\n",
    "        モデルと設定のタプル\n",
    "    \"\"\"\n",
    "    config_path = STYLETTS2_DIR / \"Configs\" / f\"config_jvs002_{dataset_name}.yml\"\n",
    "    model_dir = STYLETTS2_DIR / \"Models\" / f\"jvs002_{dataset_name}\"\n",
    "    \n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(f\"設定ファイルが見つかりません: {config_path}\")\n",
    "    if not model_dir.exists():\n",
    "        raise FileNotFoundError(f\"モデルディレクトリが見つかりません: {model_dir}\")\n",
    "    \n",
    "    # 設定を読み込み\n",
    "    config = yaml.safe_load(open(config_path, encoding='utf-8'))\n",
    "    \n",
    "    # 最新のStage 2チェックポイントを探す\n",
    "    checkpoint_files = list(model_dir.glob(\"epoch_2nd_*.pth\"))\n",
    "    if not checkpoint_files:\n",
    "        # Stage 1チェックポイントを探す\n",
    "        checkpoint_files = list(model_dir.glob(\"epoch_1st_*.pth\"))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        raise FileNotFoundError(f\"チェックポイントが見つかりません: {model_dir}\")\n",
    "    \n",
    "    checkpoint_path = sorted(checkpoint_files)[-1]  # 最新のものを使用\n",
    "    print(f\"  チェックポイント: {checkpoint_path.name}\")\n",
    "    \n",
    "    # ASRモデルの読み込み\n",
    "    ASR_config = config.get('ASR_config', False)\n",
    "    ASR_path = config.get('ASR_path', False)\n",
    "    if ASR_config and ASR_path:\n",
    "        ASR_config_path = STYLETTS2_DIR / ASR_config\n",
    "        ASR_model_path = STYLETTS2_DIR / ASR_path\n",
    "        text_aligner = load_ASR_models(str(ASR_model_path), str(ASR_config_path))\n",
    "    else:\n",
    "        raise ValueError(\"ASR設定が見つかりません\")\n",
    "    \n",
    "    # F0モデルの読み込み\n",
    "    F0_path = config.get('F0_path', False)\n",
    "    if F0_path:\n",
    "        F0_model_path = STYLETTS2_DIR / F0_path\n",
    "        pitch_extractor = load_F0_models(str(F0_model_path))\n",
    "    else:\n",
    "        raise ValueError(\"F0設定が見つかりません\")\n",
    "    \n",
    "    # PLBERTの読み込み\n",
    "    BERT_path = config.get('PLBERT_dir', False)\n",
    "    if BERT_path:\n",
    "        from Utils.PLBERT.util import load_plbert\n",
    "        plbert = load_plbert(str(STYLETTS2_DIR / BERT_path))\n",
    "    else:\n",
    "        raise ValueError(\"PLBERT設定が見つかりません\")\n",
    "    \n",
    "    # モデルの構築\n",
    "    model = build_model(recursive_munch(config['model_params']), text_aligner, pitch_extractor, plbert)\n",
    "    \n",
    "    # チェックポイントの読み込み\n",
    "    params_whole = torch.load(str(checkpoint_path), map_location='cpu')\n",
    "    params = params_whole.get('net', params_whole)\n",
    "    \n",
    "    for key in model:\n",
    "        if key in params:\n",
    "            try:\n",
    "                model[key].load_state_dict(params[key])\n",
    "            except:\n",
    "                from collections import OrderedDict\n",
    "                state_dict = params[key]\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in state_dict.items():\n",
    "                    name = k[7:] if k.startswith('module.') else k  # module.を削除\n",
    "                    new_state_dict[name] = v\n",
    "                model[key].load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    # モデルを評価モードに\n",
    "    _ = [model[key].eval() for key in model]\n",
    "    _ = [model[key].to(device) for key in model]\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "print(\"✓ モデル読み込み関数を定義しました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声生成関数\n",
    "def synthesize_speech(model, text: str, ref_audio_path: str = None):\n",
    "    \"\"\"\n",
    "    テキストから音声を生成\n",
    "    \n",
    "    Args:\n",
    "        model: 学習済みモデル\n",
    "        text: 入力テキスト\n",
    "        ref_audio_path: 参照音声ファイルパス（スタイル参照用、オプション）\n",
    "        \n",
    "    Returns:\n",
    "        生成された音声波形（numpy配列）\n",
    "    \"\"\"\n",
    "    # テキストを音素化\n",
    "    phonemized = phonemize_japanese(text)\n",
    "    \n",
    "    # テキストをトークン化\n",
    "    tokens = textcleaner(phonemized)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens.append(0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "    \n",
    "    # 参照音声の読み込み（スタイル参照用）\n",
    "    if ref_audio_path and Path(ref_audio_path).exists():\n",
    "        ref_wav, sr = librosa.load(ref_audio_path, sr=24000)\n",
    "        ref_wav = torch.from_numpy(ref_wav).unsqueeze(0).to(device)\n",
    "    else:\n",
    "        # デフォルトのスタイル（ゼロベクトル）\n",
    "        ref_wav = None\n",
    "    \n",
    "    # メルスペクトログラムの前処理\n",
    "    to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "        n_mels=80, n_fft=2048, win_length=1200, hop_length=300).to(device)\n",
    "    mean, std = -4, 4\n",
    "    \n",
    "    def preprocess(wave):\n",
    "        if isinstance(wave, np.ndarray):\n",
    "            wave = torch.from_numpy(wave).float()\n",
    "        mel_tensor = to_mel(wave)\n",
    "        mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "        return mel_tensor\n",
    "    \n",
    "    # スタイルベクトルの計算\n",
    "    if ref_wav is not None:\n",
    "        with torch.no_grad():\n",
    "            ref_mel = preprocess(ref_wav.squeeze(0).cpu().numpy())\n",
    "            style = model.style_encoder(ref_mel.unsqueeze(1).to(device))\n",
    "    else:\n",
    "        # デフォルトスタイル\n",
    "        style = torch.zeros(1, 128).to(device)\n",
    "    \n",
    "    # Diffusion Samplerの設定\n",
    "    from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "    sampler = DiffusionSampler(\n",
    "        model.diffusion.diffusion,\n",
    "        sampler=ADPM2Sampler(),\n",
    "        sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0),\n",
    "        clamp=False\n",
    "    )\n",
    "    \n",
    "    # 音声生成\n",
    "    with torch.no_grad():\n",
    "        # BERT埋め込み\n",
    "        bert_dur = model.bert(tokens, return_mask=False)\n",
    "        \n",
    "        # デュレーションモデル\n",
    "        d = model.predictor.text_embedding(bert_dur)\n",
    "        d = model.predictor.lstm(d)\n",
    "        d = d.transpose(-1, -2)\n",
    "        d = model.predictor.duration_proj(d)\n",
    "        d = d.squeeze()\n",
    "        \n",
    "        pred_dur = torch.round(d).long()\n",
    "        pred_dur = torch.clamp(pred_dur, min=1)\n",
    "        \n",
    "        # アライメント\n",
    "        pred_aln_trg = torch.zeros(pred_dur.shape[0], pred_dur.sum()).to(device)\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.shape[0]):\n",
    "            pred_aln_trg[i, c_frame:c_frame + pred_dur[i]] = 1\n",
    "            c_frame += pred_dur[i]\n",
    "        \n",
    "        # スタイル拡散\n",
    "        en = model.predictor.en(tokens.unsqueeze(0))\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, style.squeeze(1))\n",
    "        \n",
    "        out = sampler(noise=torch.randn((1, 128)).unsqueeze(1).to(device),\n",
    "                      embedding=bert_dur,\n",
    "                      embedding_scale=1.0,\n",
    "                      features=F0_pred,\n",
    "                      num_steps=5).squeeze(1)\n",
    "        \n",
    "        # デコーダー\n",
    "        mel = model.decoder(out, F0_pred, N_pred, tokens.unsqueeze(0), pred_aln_trg.unsqueeze(0))\n",
    "        \n",
    "        # ボコーダー（mel to waveform）\n",
    "        wav = model.vocoder(mel.permute(0, 1, 2))\n",
    "    \n",
    "    # numpy配列に変換\n",
    "    wav_np = wav.squeeze().cpu().numpy()\n",
    "    \n",
    "    return wav_np\n",
    "\n",
    "print(\"✓ 音声生成関数を定義しました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データセットで評価音声を生成\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"評価音声生成開始\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"データセット: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # モデルの読み込み\n",
    "        print(\"モデルを読み込んでいます...\")\n",
    "        model, config = load_model(dataset_name)\n",
    "        print(\"✓ モデルの読み込み完了\")\n",
    "        \n",
    "        # 出力ディレクトリの作成\n",
    "        output_dir = OUTPUT_ROOT / \"eval_audio\" / f\"jvs002_{dataset_name}\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 参照音声の取得（データセットから1つ選ぶ）\n",
    "        data_dir = STYLETTS2_DIR / \"Data\" / f\"jvs002_{dataset_name}\" / \"wavs\"\n",
    "        ref_audio_files = list(data_dir.glob(\"*.wav\"))\n",
    "        ref_audio_path = str(ref_audio_files[0]) if ref_audio_files else None\n",
    "        \n",
    "        if ref_audio_path:\n",
    "            print(f\"参照音声: {Path(ref_audio_path).name}\")\n",
    "        \n",
    "        # 各評価文で音声生成\n",
    "        for i, text in enumerate(EVAL_TEXTS, 1):\n",
    "            print(f\"\\n評価文 {i}/{len(EVAL_TEXTS)}: {text[:30]}...\")\n",
    "            \n",
    "            try:\n",
    "                wav = synthesize_speech(model, text, ref_audio_path)\n",
    "                \n",
    "                # 音声ファイルの保存\n",
    "                output_path = output_dir / f\"eval_{i:03d}.wav\"\n",
    "                sf.write(str(output_path), wav, sr=24000)\n",
    "                print(f\"✓ 保存: {output_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ エラー: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        print(f\"\\n✓ {dataset_name} の評価音声生成完了\")\n",
    "        print(f\"出力ディレクトリ: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ {dataset_name} の処理に失敗: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成された音声ファイルの確認\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"生成された音声ファイルの確認\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    output_dir = OUTPUT_ROOT / \"eval_audio\" / f\"jvs002_{dataset_name}\"\n",
    "    \n",
    "    if output_dir.exists():\n",
    "        audio_files = sorted(output_dir.glob(\"eval_*.wav\"))\n",
    "        print(f\"\\n{dataset_name}: {len(audio_files)}ファイル\")\n",
    "        for audio_file in audio_files:\n",
    "            print(f\"  - {audio_file.name}\")\n",
    "    else:\n",
    "        print(f\"\\n{dataset_name}: 出力ディレクトリが存在しません\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価音声生成完了\n",
    "\n",
    "各データセットの評価音声が `outputs/eval_audio/jvs002_{dataset_name}/` に保存されました。\n",
    "\n",
    "これらの音声ファイルを使って、各データセット条件での音声品質を比較・評価できます。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.12.9 ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}