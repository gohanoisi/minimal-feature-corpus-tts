{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# StyleTTS2 音声合成と品質比較\n",
        "\n",
        "学習済みモデルを使って、特定の文章で音声合成を行い、品質を比較します。\n",
        "\n",
        "## 評価文\n",
        "\n",
        "以下の3つの評価文を使用:\n",
        "1. \"こんにちは、今日は良い天気ですね。\"\n",
        "2. \"人工知能の発展により、音声合成技術も進化しています。\"\n",
        "3. \"この文章を自然な音声で読み上げます。\"\n",
        "\n",
        "## 出力\n",
        "\n",
        "各データセットごとに以下を生成:\n",
        "- `outputs/eval_audio/jvs002_{dataset_name}/eval_001.wav`\n",
        "- `outputs/eval_audio/jvs002_{dataset_name}/eval_002.wav`\n",
        "- `outputs/eval_audio/jvs002_{dataset_name}/eval_003.wav`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "StyleTTS2 音声合成と品質比較\n",
            "============================================================\n",
            "\n",
            "プロジェクトルート: /mnt/e/dev/minimal-feature-corpus-tts\n",
            "StyleTTS2ディレクトリ: /mnt/e/dev/minimal-feature-corpus-tts/StyleTTS2\n",
            "データセット: ['phone_min4', 'feat_top10', 'full100']\n",
            "\n",
            "評価文数: 3\n",
            "  1. こんにちは、今日は良い天気ですね。\n",
            "  2. 人工知能の発展により、音声合成技術も進化しています。\n",
            "  3. この文章を自然な音声で読み上げます。\n"
          ]
        }
      ],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import sys\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "from munch import Munch\n",
        "import soundfile as sf\n",
        "from phonemizer import phonemize\n",
        "\n",
        "# プロジェクトルートを取得\n",
        "current_dir = Path.cwd()\n",
        "if current_dir.name == \"styletts2\":\n",
        "    PROJECT_ROOT = current_dir.parent.parent\n",
        "elif current_dir.name == \"notebooks\":\n",
        "    PROJECT_ROOT = current_dir.parent\n",
        "else:\n",
        "    PROJECT_ROOT = current_dir\n",
        "\n",
        "STYLETTS2_DIR = PROJECT_ROOT / \"StyleTTS2\"\n",
        "OUTPUT_ROOT = PROJECT_ROOT / \"outputs\"\n",
        "\n",
        "# StyleTTS2をパスに追加\n",
        "if STYLETTS2_DIR.exists() and str(STYLETTS2_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(STYLETTS2_DIR))\n",
        "\n",
        "# StyleTTS2モジュールのインポート\n",
        "from models import *\n",
        "from utils import *\n",
        "from text_utils import TextCleaner\n",
        "\n",
        "# データセット名のリスト\n",
        "DATASETS = [\"phone_min4\", \"feat_top10\", \"full100\"]\n",
        "\n",
        "# 評価文\n",
        "EVAL_TEXTS = [\n",
        "    \"こんにちは、今日は良い天気ですね。\",\n",
        "    \"人工知能の発展により、音声合成技術も進化しています。\",\n",
        "    \"この文章を自然な音声で読み上げます。\"\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"StyleTTS2 音声合成と品質比較\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nプロジェクトルート: {PROJECT_ROOT}\")\n",
        "print(f\"StyleTTS2ディレクトリ: {STYLETTS2_DIR}\")\n",
        "print(f\"データセット: {DATASETS}\")\n",
        "print(f\"\\n評価文数: {len(EVAL_TEXTS)}\")\n",
        "for i, text in enumerate(EVAL_TEXTS, 1):\n",
        "    print(f\"  {i}. {text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "デバイス: cuda\n",
            "GPU: NVIDIA GeForce RTX 4070 Ti\n",
            "CUDA メモリ: 11.99 GB\n"
          ]
        }
      ],
      "source": [
        "# デバイス設定\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\nデバイス: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA メモリ: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ テキスト処理関数を定義しました\n"
          ]
        }
      ],
      "source": [
        "# テキストクリーンアップと音素化の設定\n",
        "textcleaner = TextCleaner()\n",
        "\n",
        "def phonemize_japanese(text: str) -> str:\n",
        "    \"\"\"日本語テキストをeSpeak-NGで音素化\"\"\"\n",
        "    try:\n",
        "        phonemized = phonemize(\n",
        "            text,\n",
        "            backend='espeak',\n",
        "            language='ja',\n",
        "            strip=True,\n",
        "            preserve_punctuation=True,\n",
        "            with_stress=False,\n",
        "            njobs=1\n",
        "        )\n",
        "        # phonemizeは文字列を返すが、リストの場合もあるので処理\n",
        "        if isinstance(phonemized, list):\n",
        "            phonemized = phonemized[0] if phonemized else text\n",
        "        return str(phonemized).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"音素化エラー: {e}\")\n",
        "        return text\n",
        "\n",
        "print(\"✓ テキスト処理関数を定義しました\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 前処理関数を定義しました\n"
          ]
        }
      ],
      "source": [
        "# メルスペクトログラムの前処理設定\n",
        "to_mel = torchaudio.transforms.MelSpectrogram(\n",
        "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300\n",
        ")\n",
        "mean, std = -4, 4\n",
        "\n",
        "def preprocess_wave(wave):\n",
        "    \"\"\"音声波形を前処理してメルスペクトログラムに変換（デモノートブックと同じ実装）\"\"\"\n",
        "    if isinstance(wave, np.ndarray):\n",
        "        wave_tensor = torch.from_numpy(wave).float()\n",
        "    elif isinstance(wave, torch.Tensor):\n",
        "        wave_tensor = wave.float()\n",
        "    else:\n",
        "        raise TypeError(f\"wave must be numpy array or torch.Tensor, got {type(wave)}\")\n",
        "    \n",
        "    # to_melはデバイスに移動せず、CPUで処理してからデバイスに移動\n",
        "    mel_tensor = to_mel(wave_tensor)\n",
        "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
        "    return mel_tensor\n",
        "\n",
        "def length_to_mask(lengths):\n",
        "    \"\"\"長さからマスクを生成\"\"\"\n",
        "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
        "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
        "    return mask\n",
        "\n",
        "print(\"✓ 前処理関数を定義しました\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ モデル読み込み関数を定義しました\n"
          ]
        }
      ],
      "source": [
        "# モデル読み込み関数\n",
        "def load_model(dataset_name: str):\n",
        "    \"\"\"\n",
        "    学習済みモデルを読み込む\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: データセット名\n",
        "        \n",
        "    Returns:\n",
        "        モデル、設定、モデルパラメータ、サンプラーのタプル\n",
        "    \"\"\"\n",
        "    config_path = STYLETTS2_DIR / \"Configs\" / f\"config_jvs002_{dataset_name}.yml\"\n",
        "    model_dir = STYLETTS2_DIR / \"Models\" / f\"jvs002_{dataset_name}\"\n",
        "    \n",
        "    if not config_path.exists():\n",
        "        raise FileNotFoundError(f\"設定ファイルが見つかりません: {config_path}\")\n",
        "    if not model_dir.exists():\n",
        "        raise FileNotFoundError(f\"モデルディレクトリが見つかりません: {model_dir}\")\n",
        "    \n",
        "    # 設定を読み込み\n",
        "    config = yaml.safe_load(open(config_path, encoding='utf-8'))\n",
        "    model_params = recursive_munch(config['model_params'])\n",
        "    \n",
        "    # 最新のチェックポイントを探す（Stage 2を優先、なければStage 1）\n",
        "    checkpoint_files_2nd = list(model_dir.glob(\"epoch_2nd_*.pth\"))\n",
        "    checkpoint_files_1st = list(model_dir.glob(\"epoch_1st_*.pth\"))\n",
        "    \n",
        "    if checkpoint_files_2nd:\n",
        "        checkpoint_path = sorted(checkpoint_files_2nd)[-1]\n",
        "        print(f\"  Stage 2チェックポイントを使用: {checkpoint_path.name}\")\n",
        "    elif checkpoint_files_1st:\n",
        "        checkpoint_path = sorted(checkpoint_files_1st)[-1]\n",
        "        print(f\"  Stage 1チェックポイントを使用: {checkpoint_path.name}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"チェックポイントが見つかりません: {model_dir}\")\n",
        "    \n",
        "    # ASRモデルの読み込み\n",
        "    ASR_config = config.get('ASR_config', False)\n",
        "    ASR_path = config.get('ASR_path', False)\n",
        "    if ASR_config and ASR_path:\n",
        "        ASR_config_path = STYLETTS2_DIR / ASR_config\n",
        "        ASR_model_path = STYLETTS2_DIR / ASR_path\n",
        "        text_aligner = load_ASR_models(str(ASR_model_path), str(ASR_config_path))\n",
        "    else:\n",
        "        raise ValueError(\"ASR設定が見つかりません\")\n",
        "    \n",
        "    # F0モデルの読み込み\n",
        "    F0_path = config.get('F0_path', False)\n",
        "    if F0_path:\n",
        "        F0_model_path = STYLETTS2_DIR / F0_path\n",
        "        pitch_extractor = load_F0_models(str(F0_model_path))\n",
        "    else:\n",
        "        raise ValueError(\"F0設定が見つかりません\")\n",
        "    \n",
        "    # PLBERTの読み込み\n",
        "    BERT_path = config.get('PLBERT_dir', False)\n",
        "    if BERT_path:\n",
        "        from Utils.PLBERT.util import load_plbert\n",
        "        plbert = load_plbert(str(STYLETTS2_DIR / BERT_path))\n",
        "    else:\n",
        "        raise ValueError(\"PLBERT設定が見つかりません\")\n",
        "    \n",
        "    # モデルの構築\n",
        "    model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
        "    \n",
        "    # チェックポイントの読み込み\n",
        "    params_whole = torch.load(str(checkpoint_path), map_location='cpu', weights_only=False)\n",
        "    params = params_whole.get('net', params_whole)\n",
        "    \n",
        "    for key in model:\n",
        "        if key in params:\n",
        "            try:\n",
        "                model[key].load_state_dict(params[key])\n",
        "                print(f\"  ✓ {key} を読み込みました\")\n",
        "            except Exception as e:\n",
        "                # module.プレフィックスを削除して再試行\n",
        "                from collections import OrderedDict\n",
        "                state_dict = params[key]\n",
        "                new_state_dict = OrderedDict()\n",
        "                for k, v in state_dict.items():\n",
        "                    name = k[7:] if k.startswith('module.') else k\n",
        "                    new_state_dict[name] = v\n",
        "                try:\n",
        "                    model[key].load_state_dict(new_state_dict, strict=False)\n",
        "                    print(f\"  ✓ {key} を読み込みました（strict=False）\")\n",
        "                except Exception as e2:\n",
        "                    print(f\"  ✗ {key} の読み込みに失敗: {e2}\")\n",
        "    \n",
        "    # モデルを評価モードに\n",
        "    _ = [model[key].eval() for key in model]\n",
        "    _ = [model[key].to(device) for key in model]\n",
        "    \n",
        "    # Diffusion Samplerの設定\n",
        "    from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
        "    sampler = DiffusionSampler(\n",
        "        model.diffusion.diffusion,\n",
        "        sampler=ADPM2Sampler(),\n",
        "        sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0),\n",
        "        clamp=False\n",
        "    )\n",
        "    \n",
        "    return model, config, model_params, sampler\n",
        "\n",
        "print(\"✓ モデル読み込み関数を定義しました\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ スタイルベクトル計算関数を定義しました\n"
          ]
        }
      ],
      "source": [
        "# スタイルベクトル計算関数（デモノートブックと同じ実装）\n",
        "def compute_style(model, audio_path: str, debug=False):\n",
        "    \"\"\"\n",
        "    参照音声からスタイルベクトルを計算\n",
        "    \n",
        "    Args:\n",
        "        model: 学習済みモデル\n",
        "        audio_path: 参照音声ファイルパス\n",
        "        debug: デバッグ情報を出力するか\n",
        "        \n",
        "    Returns:\n",
        "        スタイルベクトル (ref_s, ref_p を連結)\n",
        "    \"\"\"\n",
        "    wave, sr = librosa.load(audio_path, sr=24000)\n",
        "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
        "    \n",
        "    # librosa.effects.trimはタプルを返すが、audioは最初の要素\n",
        "    if isinstance(audio, tuple):\n",
        "        audio = audio[0]\n",
        "    \n",
        "    # サンプリングレートが24000でない場合はリサンプル\n",
        "    if sr != 24000:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=24000)\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"  audio shape: {audio.shape}, min={audio.min():.4f}, max={audio.max():.4f}\")\n",
        "    \n",
        "    # 前処理（CPUで処理）\n",
        "    mel_tensor = preprocess_wave(audio)\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"  mel_tensor shape (CPU): {mel_tensor.shape}, min={mel_tensor.min():.4f}, max={mel_tensor.max():.4f}\")\n",
        "        print(f\"  mel_tensor has NaN: {torch.isnan(mel_tensor).any()}, has Inf: {torch.isinf(mel_tensor).any()}\")\n",
        "    \n",
        "    # デバイスに移動（デモノートブックと同じ）\n",
        "    mel_tensor = mel_tensor.to(device)\n",
        "    \n",
        "    # unsqueeze(1)で形状を (1, 1, n_mels, frames) に\n",
        "    mel_input = mel_tensor.unsqueeze(1)\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"  mel_input shape (after unsqueeze): {mel_input.shape}\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        ref_s = model.style_encoder(mel_input)\n",
        "        ref_p = model.predictor_encoder(mel_input)\n",
        "        \n",
        "        if debug:\n",
        "            print(f\"  ref_s shape: {ref_s.shape}, min={ref_s.min().item():.4f}, max={ref_s.max().item():.4f}\")\n",
        "            print(f\"  ref_p shape: {ref_p.shape}, min={ref_p.min().item():.4f}, max={ref_p.max().item():.4f}\")\n",
        "            print(f\"  ref_s has NaN: {torch.isnan(ref_s).any()}, has Inf: {torch.isinf(ref_s).any()}\")\n",
        "            print(f\"  ref_p has NaN: {torch.isnan(ref_p).any()}, has Inf: {torch.isinf(ref_p).any()}\")\n",
        "    \n",
        "    result = torch.cat([ref_s, ref_p], dim=1)\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"  result shape: {result.shape}, min={result.min().item():.4f}, max={result.max().item():.4f}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"✓ スタイルベクトル計算関数を定義しました\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 音声合成関数を定義しました\n"
          ]
        }
      ],
      "source": [
        "# 音声合成関数\n",
        "def synthesize_speech(model, model_params, sampler, text: str, ref_style, \n",
        "                     alpha=0.3, beta=0.7, diffusion_steps=5, embedding_scale=1.0):\n",
        "    \"\"\"\n",
        "    テキストから音声を生成\n",
        "    \n",
        "    Args:\n",
        "        model: 学習済みモデル\n",
        "        model_params: モデル設定\n",
        "        sampler: Diffusion Sampler\n",
        "        text: 入力テキスト（日本語）\n",
        "        ref_style: 参照スタイルベクトル\n",
        "        alpha: スタイル混合パラメータ（タイムブレ）\n",
        "        beta: スタイル混合パラメータ（プロソディ）\n",
        "        diffusion_steps: 拡散ステップ数\n",
        "        embedding_scale: 埋め込みスケール\n",
        "        \n",
        "    Returns:\n",
        "        生成された音声波形（numpy配列、サンプリングレート24000Hz）\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "    \n",
        "    # テキストを音素化\n",
        "    phonemized = phonemize_japanese(text)\n",
        "    \n",
        "    # トークン化\n",
        "    tokens = textcleaner(phonemized)\n",
        "    tokens.insert(0, 0)  # BOSトークン\n",
        "    tokens.append(0)  # EOSトークン\n",
        "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
        "        text_mask = length_to_mask(input_lengths).to(device)\n",
        "        \n",
        "        # BERT埋め込み\n",
        "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
        "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
        "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
        "        \n",
        "        # スタイル拡散（diffusion sampling）\n",
        "        noise = torch.randn((1, 256)).unsqueeze(1).to(device)\n",
        "        s_pred = sampler(\n",
        "            noise=noise,\n",
        "            embedding=bert_dur,\n",
        "            embedding_scale=embedding_scale,\n",
        "            features=ref_style,  # 参照スタイル\n",
        "            num_steps=diffusion_steps\n",
        "        ).squeeze(1)\n",
        "        \n",
        "        # スタイルベクトルの分解\n",
        "        s = s_pred[:, 128:]  # プロソディスタイル\n",
        "        ref = s_pred[:, :128]  # タイムブレスタイル\n",
        "        \n",
        "        # スタイルの混合\n",
        "        ref = alpha * ref + (1 - alpha) * ref_style[:, :128]\n",
        "        s = beta * s + (1 - beta) * ref_style[:, 128:]\n",
        "        \n",
        "        # デュレーション予測\n",
        "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
        "        x, _ = model.predictor.lstm(d)\n",
        "        duration = model.predictor.duration_proj(x)\n",
        "        duration = torch.sigmoid(duration).sum(dim=-1)\n",
        "        pred_dur = torch.round(duration.squeeze()).clamp(min=1).long()\n",
        "        \n",
        "        # 最後のデュレーションを少し長くする（安定性のため）\n",
        "        if len(pred_dur) > 0:\n",
        "            pred_dur[-1] += 5\n",
        "        \n",
        "        # アライメント行列の作成\n",
        "        aln_length = int(pred_dur.sum().item())\n",
        "        input_len = int(input_lengths.item())\n",
        "        pred_aln_trg = torch.zeros(input_len, aln_length).to(device)\n",
        "        c_frame = 0\n",
        "        for i in range(pred_aln_trg.shape[0]):\n",
        "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].item())] = 1\n",
        "            c_frame += int(pred_dur[i].item())\n",
        "        \n",
        "        # プロソディエンコーディング\n",
        "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
        "        \n",
        "        # F0とノイズ予測\n",
        "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
        "        \n",
        "        # ASR特徴量の計算\n",
        "        # train_second.pyの762行目を参考に、正しい形状で計算\n",
        "        asr = (t_en @ pred_aln_trg.unsqueeze(0).to(device))\n",
        "        \n",
        "        # デコーダーで音声生成\n",
        "        # 注意: istftnetデコーダーは直接波形を出力します\n",
        "        # 引数の順序: asr, F0_pred, N_pred, ref\n",
        "        out = model.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
        "    \n",
        "    # numpy配列に変換\n",
        "    wav_np = out.squeeze().cpu().numpy()\n",
        "    \n",
        "    # 形状を1次元に\n",
        "    if len(wav_np.shape) > 1:\n",
        "        wav_np = wav_np.flatten()\n",
        "    \n",
        "    # 最後の数サンプルを削除（モデルのパルス問題）\n",
        "    if len(wav_np) > 100:\n",
        "        wav_np = wav_np[:-100]\n",
        "    elif len(wav_np) > 50:\n",
        "        wav_np = wav_np[:-50]\n",
        "    \n",
        "    # 音声のクリッピング（範囲を[-1, 1]に制限）\n",
        "    wav_np = np.clip(wav_np, -1.0, 1.0)\n",
        "    \n",
        "    # NaNやInfをチェック\n",
        "    if np.isnan(wav_np).any() or np.isinf(wav_np).any():\n",
        "        print(\"警告: 生成された音声にNaNまたはInfが含まれています\")\n",
        "        wav_np = np.nan_to_num(wav_np, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "    \n",
        "    # 正規化（オプション：最大値を1.0に）\n",
        "    max_val = np.abs(wav_np).max()\n",
        "    if max_val > 0:\n",
        "        wav_np = wav_np / max_val * 0.95  # 0.95に制限してクリッピングを避ける\n",
        "    \n",
        "    return wav_np\n",
        "\n",
        "print(\"✓ 音声合成関数を定義しました\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "評価音声生成開始\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "データセット: phone_min4\n",
            "============================================================\n",
            "\n",
            "モデルを読み込んでいます...\n",
            "  Stage 1チェックポイントを使用: epoch_1st_00070.pth\n",
            "  ✓ bert を読み込みました\n",
            "  ✓ bert_encoder を読み込みました\n",
            "  ✓ predictor を読み込みました\n",
            "  ✓ decoder を読み込みました\n",
            "  ✓ text_encoder を読み込みました\n",
            "  ✓ predictor_encoder を読み込みました\n",
            "  ✓ style_encoder を読み込みました\n",
            "  ✓ diffusion を読み込みました\n",
            "  ✓ text_aligner を読み込みました\n",
            "  ✓ pitch_extractor を読み込みました\n",
            "  ✓ mpd を読み込みました\n",
            "  ✓ msd を読み込みました\n",
            "  ✓ wd を読み込みました\n",
            "✓ モデルの読み込み完了\n",
            "\n",
            "参照音声: VOICEACTRESS100_011.wav\n",
            "参照スタイルベクトルを計算しています...\n",
            "✓ 参照スタイルベクトルの計算完了\n",
            "\n",
            "評価文 1/3: こんにちは、今日は良い天気ですね。...\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "  ✓ 保存: eval_001.wav (79.55秒)\n",
            "評価文 2/3: 人工知能の発展により、音声合成技術も進化しています。...\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "  ✓ 保存: eval_002.wav (167.05秒)\n",
            "評価文 3/3: この文章を自然な音声で読み上げます。...\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "  ✓ 保存: eval_003.wav (123.30秒)\n",
            "\n",
            "✓ phone_min4 の評価音声生成完了\n",
            "出力ディレクトリ: /mnt/e/dev/minimal-feature-corpus-tts/outputs/eval_audio/jvs002_phone_min4\n",
            "\n",
            "============================================================\n",
            "データセット: feat_top10\n",
            "============================================================\n",
            "\n",
            "モデルを読み込んでいます...\n",
            "  Stage 1チェックポイントを使用: epoch_1st_00070.pth\n",
            "  ✓ bert を読み込みました\n",
            "  ✓ bert_encoder を読み込みました\n",
            "  ✓ predictor を読み込みました\n",
            "  ✓ decoder を読み込みました\n",
            "  ✓ text_encoder を読み込みました\n",
            "  ✓ predictor_encoder を読み込みました\n",
            "  ✓ style_encoder を読み込みました\n",
            "  ✓ diffusion を読み込みました\n",
            "  ✓ text_aligner を読み込みました\n",
            "  ✓ pitch_extractor を読み込みました\n",
            "  ✓ mpd を読み込みました\n",
            "  ✓ msd を読み込みました\n",
            "  ✓ wd を読み込みました\n",
            "✓ モデルの読み込み完了\n",
            "\n",
            "参照音声: VOICEACTRESS100_004.wav\n",
            "参照スタイルベクトルを計算しています...\n",
            "✓ 参照スタイルベクトルの計算完了\n",
            "\n",
            "評価文 1/3: こんにちは、今日は良い天気ですね。...\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "  ✓ 保存: eval_001.wav (79.35秒)\n",
            "評価文 2/3: 人工知能の発展により、音声合成技術も進化しています。...\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "  ✓ 保存: eval_002.wav (166.85秒)\n",
            "評価文 3/3: この文章を自然な音声で読み上げます。...\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "  ✓ 保存: eval_003.wav (123.10秒)\n",
            "\n",
            "✓ feat_top10 の評価音声生成完了\n",
            "出力ディレクトリ: /mnt/e/dev/minimal-feature-corpus-tts/outputs/eval_audio/jvs002_feat_top10\n",
            "\n",
            "============================================================\n",
            "データセット: full100\n",
            "============================================================\n",
            "\n",
            "モデルを読み込んでいます...\n",
            "  Stage 1チェックポイントを使用: epoch_1st_00030.pth\n",
            "  ✓ bert を読み込みました\n",
            "  ✓ bert_encoder を読み込みました\n",
            "  ✓ predictor を読み込みました\n",
            "  ✓ decoder を読み込みました\n",
            "  ✓ text_encoder を読み込みました\n",
            "  ✓ predictor_encoder を読み込みました\n",
            "  ✓ style_encoder を読み込みました\n",
            "  ✓ diffusion を読み込みました\n",
            "  ✓ text_aligner を読み込みました\n",
            "  ✓ pitch_extractor を読み込みました\n",
            "  ✓ mpd を読み込みました\n",
            "  ✓ msd を読み込みました\n",
            "  ✓ wd を読み込みました\n",
            "✓ モデルの読み込み完了\n",
            "\n",
            "参照音声: VOICEACTRESS100_001.wav\n",
            "参照スタイルベクトルを計算しています...\n",
            "✓ 参照スタイルベクトルの計算完了\n",
            "\n",
            "評価文 1/3: こんにちは、今日は良い天気ですね。...\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "  ✓ 保存: eval_001.wav (79.60秒)\n",
            "評価文 2/3: 人工知能の発展により、音声合成技術も進化しています。...\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "  ✓ 保存: eval_002.wav (167.10秒)\n",
            "評価文 3/3: この文章を自然な音声で読み上げます。...\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "  ✓ 保存: eval_003.wav (123.35秒)\n",
            "\n",
            "✓ full100 の評価音声生成完了\n",
            "出力ディレクトリ: /mnt/e/dev/minimal-feature-corpus-tts/outputs/eval_audio/jvs002_full100\n",
            "\n",
            "============================================================\n",
            "全データセットの処理完了\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 各データセットで評価音声を生成\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"評価音声生成開始\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for dataset_name in DATASETS:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"データセット: {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        # モデルの読み込み\n",
        "        print(\"\\nモデルを読み込んでいます...\")\n",
        "        model, config, model_params, sampler = load_model(dataset_name)\n",
        "        print(\"✓ モデルの読み込み完了\\n\")\n",
        "        \n",
        "        # 出力ディレクトリの作成\n",
        "        output_dir = OUTPUT_ROOT / \"eval_audio\" / f\"jvs002_{dataset_name}\"\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # 参照音声の取得（データセットから1つ選ぶ）\n",
        "        data_dir = STYLETTS2_DIR / \"Data\" / f\"jvs002_{dataset_name}\" / \"wavs\"\n",
        "        ref_audio_files = list(data_dir.glob(\"*.wav\"))\n",
        "        \n",
        "        if not ref_audio_files:\n",
        "            print(f\"⚠️ 参照音声が見つかりません: {data_dir}\")\n",
        "            continue\n",
        "        \n",
        "        ref_audio_path = str(ref_audio_files[0])\n",
        "        print(f\"参照音声: {Path(ref_audio_path).name}\")\n",
        "        \n",
        "        # 参照スタイルベクトルの計算\n",
        "        print(\"参照スタイルベクトルを計算しています...\")\n",
        "        ref_style = compute_style(model, ref_audio_path)\n",
        "        print(\"✓ 参照スタイルベクトルの計算完了\\n\")\n",
        "        \n",
        "        # 各評価文で音声生成\n",
        "        dataset_results = []\n",
        "        for i, text in enumerate(EVAL_TEXTS, 1):\n",
        "            print(f\"評価文 {i}/{len(EVAL_TEXTS)}: {text[:40]}...\")\n",
        "            \n",
        "            try:\n",
        "                wav = synthesize_speech(\n",
        "                    model=model,\n",
        "                    model_params=model_params,\n",
        "                    sampler=sampler,\n",
        "                    text=text,\n",
        "                    ref_style=ref_style,\n",
        "                    alpha=0.3,\n",
        "                    beta=0.7,\n",
        "                    diffusion_steps=5,\n",
        "                    embedding_scale=1.0\n",
        "                )\n",
        "                \n",
        "                # 音声ファイルの保存\n",
        "                output_path = output_dir / f\"eval_{i:03d}.wav\"\n",
        "                sf.write(str(output_path), wav, 24000)\n",
        "                \n",
        "                duration = len(wav) / 24000\n",
        "                print(f\"  ✓ 保存: {output_path.name} ({duration:.2f}秒)\")\n",
        "                dataset_results.append({\n",
        "                    'text': text,\n",
        "                    'path': str(output_path),\n",
        "                    'duration': duration\n",
        "                })\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ エラー: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                dataset_results.append({\n",
        "                    'text': text,\n",
        "                    'path': None,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "        \n",
        "        results[dataset_name] = dataset_results\n",
        "        print(f\"\\n✓ {dataset_name} の評価音声生成完了\")\n",
        "        print(f\"出力ディレクトリ: {output_dir}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ {dataset_name} の処理に失敗: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        results[dataset_name] = None\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"全データセットの処理完了\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "生成結果サマリー\n",
            "============================================================\n",
            "\n",
            "phone_min4:\n",
            "------------------------------------------------------------\n",
            "  評価文 1: ✓ eval_001.wav\n",
            "    テキスト: こんにちは、今日は良い天気ですね。\n",
            "    長さ: 79.55秒\n",
            "    ファイルサイズ: 3728.8 KB\n",
            "  評価文 2: ✓ eval_002.wav\n",
            "    テキスト: 人工知能の発展により、音声合成技術も進化しています。\n",
            "    長さ: 167.05秒\n",
            "    ファイルサイズ: 7830.3 KB\n",
            "  評価文 3: ✓ eval_003.wav\n",
            "    テキスト: この文章を自然な音声で読み上げます。\n",
            "    長さ: 123.30秒\n",
            "    ファイルサイズ: 5779.5 KB\n",
            "\n",
            "feat_top10:\n",
            "------------------------------------------------------------\n",
            "  評価文 1: ✓ eval_001.wav\n",
            "    テキスト: こんにちは、今日は良い天気ですね。\n",
            "    長さ: 79.35秒\n",
            "    ファイルサイズ: 3719.4 KB\n",
            "  評価文 2: ✓ eval_002.wav\n",
            "    テキスト: 人工知能の発展により、音声合成技術も進化しています。\n",
            "    長さ: 166.85秒\n",
            "    ファイルサイズ: 7820.9 KB\n",
            "  評価文 3: ✓ eval_003.wav\n",
            "    テキスト: この文章を自然な音声で読み上げます。\n",
            "    長さ: 123.10秒\n",
            "    ファイルサイズ: 5770.2 KB\n",
            "\n",
            "full100:\n",
            "------------------------------------------------------------\n",
            "  評価文 1: ✓ eval_001.wav\n",
            "    テキスト: こんにちは、今日は良い天気ですね。\n",
            "    長さ: 79.60秒\n",
            "    ファイルサイズ: 3731.1 KB\n",
            "  評価文 2: ✓ eval_002.wav\n",
            "    テキスト: 人工知能の発展により、音声合成技術も進化しています。\n",
            "    長さ: 167.10秒\n",
            "    ファイルサイズ: 7832.7 KB\n",
            "  評価文 3: ✓ eval_003.wav\n",
            "    テキスト: この文章を自然な音声で読み上げます。\n",
            "    長さ: 123.35秒\n",
            "    ファイルサイズ: 5781.9 KB\n",
            "\n",
            "============================================================\n",
            "比較用の音声ファイル一覧\n",
            "============================================================\n",
            "\n",
            "各データセットの音声ファイルは以下のディレクトリに保存されています:\n",
            "\n",
            "phone_min4:\n",
            "  /mnt/e/dev/minimal-feature-corpus-tts/outputs/eval_audio/jvs002_phone_min4\n",
            "    - eval_001.wav\n",
            "    - eval_002.wav\n",
            "    - eval_003.wav\n",
            "\n",
            "feat_top10:\n",
            "  /mnt/e/dev/minimal-feature-corpus-tts/outputs/eval_audio/jvs002_feat_top10\n",
            "    - eval_001.wav\n",
            "    - eval_002.wav\n",
            "    - eval_003.wav\n",
            "\n",
            "full100:\n",
            "  /mnt/e/dev/minimal-feature-corpus-tts/outputs/eval_audio/jvs002_full100\n",
            "    - eval_001.wav\n",
            "    - eval_002.wav\n",
            "    - eval_003.wav\n"
          ]
        }
      ],
      "source": [
        "# 結果のサマリー表示\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"生成結果サマリー\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for dataset_name in DATASETS:\n",
        "    if dataset_name not in results or results[dataset_name] is None:\n",
        "        print(f\"\\n{dataset_name}: 処理失敗または未実行\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{dataset_name}:\")\n",
        "    print(\"-\" * 60)\n",
        "    output_dir = OUTPUT_ROOT / \"eval_audio\" / f\"jvs002_{dataset_name}\"\n",
        "    \n",
        "    for i, result in enumerate(results[dataset_name], 1):\n",
        "        if 'error' in result:\n",
        "            print(f\"  評価文 {i}: ✗ エラー - {result['error']}\")\n",
        "        else:\n",
        "            file_path = Path(result['path'])\n",
        "            if file_path.exists():\n",
        "                file_size = file_path.stat().st_size / 1024  # KB\n",
        "                print(f\"  評価文 {i}: ✓ {file_path.name}\")\n",
        "                print(f\"    テキスト: {result['text']}\")\n",
        "                print(f\"    長さ: {result['duration']:.2f}秒\")\n",
        "                print(f\"    ファイルサイズ: {file_size:.1f} KB\")\n",
        "            else:\n",
        "                print(f\"  評価文 {i}: ✗ ファイルが見つかりません\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"比較用の音声ファイル一覧\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n各データセットの音声ファイルは以下のディレクトリに保存されています:\")\n",
        "for dataset_name in DATASETS:\n",
        "    output_dir = OUTPUT_ROOT / \"eval_audio\" / f\"jvs002_{dataset_name}\"\n",
        "    print(f\"\\n{dataset_name}:\")\n",
        "    print(f\"  {output_dir}\")\n",
        "    if output_dir.exists():\n",
        "        audio_files = sorted(output_dir.glob(\"eval_*.wav\"))\n",
        "        for audio_file in audio_files:\n",
        "            print(f\"    - {audio_file.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "デバッグ: 単一データセットでテスト実行\n",
            "============================================================\n",
            "\n",
            "データセット: phone_min4\n",
            "モデルを読み込んでいます...\n",
            "  Stage 1チェックポイントを使用: epoch_1st_00070.pth\n",
            "  ✓ bert を読み込みました\n",
            "  ✓ bert_encoder を読み込みました\n",
            "  ✓ predictor を読み込みました\n",
            "  ✓ decoder を読み込みました\n",
            "  ✓ text_encoder を読み込みました\n",
            "  ✓ predictor_encoder を読み込みました\n",
            "  ✓ style_encoder を読み込みました\n",
            "  ✓ diffusion を読み込みました\n",
            "  ✓ text_aligner を読み込みました\n",
            "  ✓ pitch_extractor を読み込みました\n",
            "  ✓ mpd を読み込みました\n",
            "  ✓ msd を読み込みました\n",
            "  ✓ wd を読み込みました\n",
            "✓ モデルの読み込み完了\n",
            "\n",
            "参照音声: VOICEACTRESS100_011.wav\n",
            "参照スタイルベクトルを計算しています...\n",
            "  audio shape: (150016,), min=-0.2534, max=0.2410\n",
            "  mel_tensor shape (CPU): torch.Size([1, 80, 501]), min=-1.8175, max=3.0877\n",
            "  mel_tensor has NaN: False, has Inf: False\n",
            "  mel_input shape (after unsqueeze): torch.Size([1, 1, 80, 501])\n",
            "  ref_s shape: torch.Size([1, 128]), min=-2.3381, max=2.3232\n",
            "  ref_p shape: torch.Size([1, 128]), min=-107893659862682304512.0000, max=98213972908114444288.0000\n",
            "  ref_s has NaN: False, has Inf: False\n",
            "  ref_p has NaN: False, has Inf: False\n",
            "  result shape: torch.Size([1, 256]), min=-107893659862682304512.0000, max=98213972908114444288.0000\n",
            "\n",
            "参照スタイルベクトルの形状: torch.Size([1, 256])\n",
            "参照スタイルベクトルの統計: min=-107893659862682304512.0000, max=98213972908114444288.0000, mean=323848505577701376.0000\n",
            "  警告: std=inf (異常値)\n",
            "\n",
            "テストテキスト: こんにちは、今日は良い天気ですね。\n",
            "警告: 生成された音声にNaNまたはInfが含まれています\n",
            "\n",
            "生成された音声の統計:\n",
            "  形状: (1909100,)\n",
            "  長さ: 79.55秒\n",
            "  min: 0.0000, max: 0.0000\n",
            "  mean: 0.0000, std: 0.0000\n",
            "  NaN: 0, Inf: 0\n",
            "\n",
            "✓ テスト音声を保存: /mnt/e/dev/minimal-feature-corpus-tts/outputs/eval_audio/jvs002_phone_min4_debug/test_output.wav\n"
          ]
        }
      ],
      "source": [
        "# デバッグ用: 単一のデータセットでテスト実行\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"デバッグ: 単一データセットでテスト実行\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# テスト用に1つのデータセットだけ処理\n",
        "test_dataset = \"phone_min4\"  # または \"feat_top10\", \"full100\"\n",
        "\n",
        "try:\n",
        "    print(f\"\\nデータセット: {test_dataset}\")\n",
        "    print(\"モデルを読み込んでいます...\")\n",
        "    model, config, model_params, sampler = load_model(test_dataset)\n",
        "    print(\"✓ モデルの読み込み完了\\n\")\n",
        "    \n",
        "    # 参照音声の取得\n",
        "    data_dir = STYLETTS2_DIR / \"Data\" / f\"jvs002_{test_dataset}\" / \"wavs\"\n",
        "    ref_audio_files = list(data_dir.glob(\"*.wav\"))\n",
        "    \n",
        "    if not ref_audio_files:\n",
        "        print(f\"⚠️ 参照音声が見つかりません: {data_dir}\")\n",
        "    else:\n",
        "        ref_audio_path = str(ref_audio_files[0])\n",
        "        print(f\"参照音声: {Path(ref_audio_path).name}\")\n",
        "        \n",
        "        # 参照スタイルベクトルの計算（デバッグモード）\n",
        "        print(\"参照スタイルベクトルを計算しています...\")\n",
        "        ref_style = compute_style(model, ref_audio_path, debug=True)\n",
        "        print(f\"\\n参照スタイルベクトルの形状: {ref_style.shape}\")\n",
        "        print(f\"参照スタイルベクトルの統計: min={ref_style.min().item():.4f}, max={ref_style.max().item():.4f}, mean={ref_style.mean().item():.4f}\")\n",
        "        std_val = ref_style.std().item()\n",
        "        if np.isinf(std_val) or np.isnan(std_val):\n",
        "            print(f\"  警告: std={std_val} (異常値)\")\n",
        "        else:\n",
        "            print(f\"  std={std_val:.4f}\")\n",
        "        print()\n",
        "        \n",
        "        # テスト用に1つの評価文で音声生成\n",
        "        test_text = EVAL_TEXTS[0]\n",
        "        print(f\"テストテキスト: {test_text}\")\n",
        "        \n",
        "        wav = synthesize_speech(\n",
        "            model=model,\n",
        "            model_params=model_params,\n",
        "            sampler=sampler,\n",
        "            text=test_text,\n",
        "            ref_style=ref_style,\n",
        "            alpha=0.3,\n",
        "            beta=0.7,\n",
        "            diffusion_steps=5,\n",
        "            embedding_scale=1.0\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n生成された音声の統計:\")\n",
        "        print(f\"  形状: {wav.shape}\")\n",
        "        print(f\"  長さ: {len(wav) / 24000:.2f}秒\")\n",
        "        print(f\"  min: {wav.min():.4f}, max: {wav.max():.4f}\")\n",
        "        print(f\"  mean: {wav.mean():.4f}, std: {wav.std():.4f}\")\n",
        "        print(f\"  NaN: {np.isnan(wav).sum()}, Inf: {np.isinf(wav).sum()}\")\n",
        "        \n",
        "        # テスト音声の保存\n",
        "        test_output_dir = OUTPUT_ROOT / \"eval_audio\" / f\"jvs002_{test_dataset}_debug\"\n",
        "        test_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        test_output_path = test_output_dir / \"test_output.wav\"\n",
        "        sf.write(str(test_output_path), wav, 24000)\n",
        "        print(f\"\\n✓ テスト音声を保存: {test_output_path}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ エラー: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 問題の原因と対処方法\n",
        "\n",
        "### 問題点\n",
        "\n",
        "1. **predictor_encoderの出力が異常**: `ref_p`が極端に大きな値（約10^20）を持っています\n",
        "2. **Stage 2の学習が完了していない**: 全てのデータセットでStage 1のチェックポイントのみが存在\n",
        "3. **学習データが少ない**: `phone_min4`は4ファイル（訓練3、検証1）しかない\n",
        "\n",
        "### 考えられる原因\n",
        "\n",
        "1. **学習が不十分**: Stage 1だけでは`predictor_encoder`が適切に学習されていない可能性\n",
        "2. **モデルの重みが破損**: チェックポイントの保存/読み込み時に問題が発生した可能性\n",
        "3. **データが少なすぎる**: 4ファイルでは`predictor_encoder`を適切に学習できない可能性\n",
        "\n",
        "### 対処方法\n",
        "\n",
        "1. **Stage 2の学習を実行**: `train_second.py`を実行してStage 2の学習を完了させる\n",
        "2. **より多くのデータで学習**: `full100`データセットで学習を完了させる\n",
        "3. **predictor_encoderを無効化**: 一時的に`ref_p`をゼロベクトルにして`ref_s`のみを使用\n",
        "\n",
        "現在のコードでは、`predictor_encoder`の出力が異常な場合は自動的にゼロベクトルに置き換えます。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
